### 新增
- repeat
```python
# autograd/tensor.py
class Tensor:
    ...
    def repeat(self, n, axis=-1):
        return Tensor.cat([self] * n, axis=axis)
```
用于GQA。对cat方法进行封装达到repeat的效果。
- GQA
```python
# transformer/modern_blocks
class GroupedQueryAttention(nn.Module):
    def __init__(self, d_model, num_q_heads, num_kv_heads, dropout_p=0.1, max_len=5000):
        assert d_model % num_q_heads == 0
        assert num_q_heads % num_kv_heads == 0
        self.d_model = d_model
        self.num_q_heads = num_q_heads
        self.num_kv_heads = num_kv_heads
        self.num_groups = num_q_heads // num_kv_heads
        self.d_k = d_model // num_q_heads

        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, self.d_k * num_kv_heads)
        self.v_proj = nn.Linear(d_model, self.d_k * num_kv_heads)

        self.attn = ScaledDotProductAttention(self.d_k, dropout_p)

        self.out_proj = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout_p)
        self.norm = nn.RMSNorm(d_model)
        self.rope = RotatePositionalEncoding(self.d_k, max_len)  # rope对象

    def forward(self, query, key, value, mask=None):
        shortcut = query
        query, key, value = self.norm(query), self.norm(key), self.norm(value)  # pre-norm
        batch_size = query.shape[0]

        Q = self.q_proj(query)
        K = self.k_proj(key)
        V = self.v_proj(value)

        Q = Q.reshape(batch_size, -1, self.num_q_heads, self.d_k).transpose(0, 2, 1, 3)  # b,n,s,d_k
        K = K.reshape(batch_size, -1, self.num_kv_heads, self.d_k).transpose(0, 2, 1, 3)  # b,g,s,d_k
        V = V.reshape(batch_size, -1, self.num_kv_heads, self.d_k).transpose(0, 2, 1, 3)  # b,g,s,d_k

        Q, K = self.rope(Q), self.rope(K)

        attn_out = self.attn(Q, K.repeat(self.num_groups, 1), V.repeat(self.num_groups, 1),
                             np.expand_dims(mask, 1) if mask is not None else None)  # mask在head维度升维（用于广播）以适配多头注意力
        attn_out = attn_out.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)

        output = self.out_proj(attn_out)
        return shortcut + self.dropout(output)
```
MHA在使用kv_cache时，每个head都要单独生成一个kv张量存在显存中，会极大的占用显存。GQA将kv_head分为若干个group，每个group共享一个权重，在生成kv_cache时只需要为每个group生成一个kv_cache，降低显存占用。由于生成k和v的线性层只需要生成对应组的kv矩阵，在进行自注意力机制时需要先在head维度进行复制以匹配q矩阵的维度。
- RoPE
```python
# transformer/modern_blocks
class RotatePositionalEncoding:
    def __init__(self, dim, max_len=5000, base=10000):
        """预计算编码查找表"""
        assert dim % 2 == 0
        self.dim = dim

        # 计算频率 theta_i = base^(-2i / dim)
        inv_freq = base ** (-(np.arange(0, dim, 2, dtype=np.float32) / dim))

        # 计算位置
        pos = np.expand_dims(np.arange(max_len, dtype=np.float32), -1)  # 广播最后一个维度用于产生高阶矩阵

        # 计算角度 pos * theta_i
        freqs = pos * inv_freq

        # 计算 cos 和 sin
        self.cos_table = np.cos(freqs)
        self.sin_table = np.sin(freqs)

    def __call__(self, x):
        assert x.shape[-1] == self.dim
        cos_emb = np.expand_dims(self.cos_table[:x.shape[2]], (0, 1))
        sin_emb = np.expand_dims(self.sin_table[:x.shape[2]], (0, 1))
        # 分割x
        x1 = x[..., :self.dim // 2]
        x2 = x[..., self.dim // 2:]

        # 旋转
        rotated_x1 = x1 * cos_emb - x2 * sin_emb
        rotated_x2 = x1 * sin_emb + x2 * cos_emb
        return Tensor.cat([rotated_x1, rotated_x2], -1)
```
与transformer原始论文的直接将位置编码与词嵌入向量相加不同，RoPE将注意力机制中的q、k矩阵旋转一个与位置相关的角度，也即乘一个旋转矩阵来达到位置编码的效果。由于向量在空间中的旋转可以表现为与一个旋转矩阵的线性运算，因此非常容易被同为线性变换的自注意力层学习。  
下面简要证明一下RoPE可以反映不同token的相对位置关系：  
我们首先简要证明一下向量在空间中的旋转可以由左乘一个旋转矩阵来实现:  
对于复平面上的向量的旋转，由欧拉公式可得  
$$(x_1 + i * x_2) * e^{i\theta}=(x_1 + i * x_2)*(cos\theta+i*sin\theta)=(cos\theta-sin\theta)*x_1+i*(cos\theta+sin\theta)*x2$$  
即向量$(x_1,x_2)^T$左乘旋转矩阵$R = \begin{pmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{pmatrix}$   
观察旋转矩阵，显然有$R^T=-R$  
现在我们考虑两个向量$q_m$、$k_n$，其中m、n对应token的位置。考虑对$q_m$、$k_n$分别应用旋转矩阵$R(m\theta)$、$R(n\theta)$,考虑向量积  
$$\langle R_m q, R_n k \rangle = (R_m q)^T (R_n k) = q^T R_m^T R_n k = q^T R_{-m} R_n k = q^T R_{n-m} k$$  
因此向量积只与相对位置m-n有关。  
对于高维的词嵌入向量，我们将其分$d_model//2$组二维向量并分别对其进行对应频率的位置编码，其中频率是关于词嵌入维度的函数(与原始transformer的位置编码类似)。  
在代码中，相较于一般原理叙述的、将词嵌入向量以相邻两个为一组的方式，我们直接将词嵌入向量在词嵌入维度对半分，让$x_i$与$x_{d_model//2 + 1}$一组，这实际上是等价的，因为词嵌入维度是等价的，特别是在训练前，区别只是词嵌入维度的组织方式不同而已。对半分在工业上有一个好处:在运用旋转位置编码后简单拼接即可，而相邻两个一组在重新整合成一个向量时稍显冗余。   
此外，RoPE只对Q、K进行编码，是因为Q、K参与计算注意力分数时需要相对位置信息，而V提供的只是信息内容，与位置信息无关。这种编码方式实际上是舍弃了绝对位置信息而专注于相对位置信息的提取。  
由于位置编码不是简单的相加，不需要对Embedding层输出进行缩放。
- modern_transformer
运用现代transformer技术而组装成的“现代版”transformer。内部运用了SparseMOE(with aux loss)、RMSNorm、Pre Norm、GQA、RoPE以及GELU。