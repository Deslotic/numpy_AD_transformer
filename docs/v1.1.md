### 修改
- parameter
```python
# tensor.py
...
    @staticmethod
    def parameter(*shape, init_method='xavier'):
        """
        创建参数对象。事实上，在pytorch中，参数对象是被包装为Parameter实例的;
        在这里，我们只返回一个需要梯度的、经过初始化的Tensor实例，其本质是相同的。
        支持xavier初始化（transformer原始论文选择）、kaiming初始化（更适合relu激活函数）
        """

        # 支持1维的bias初始化以及2维的weight初始化
        # 更新：支持任意维度的参数初始化
        if len(shape) == 1:
            fan_in, fan_out = shape[0], 0
        else:
            fan_in, fan_out = shape[-2], shape[-1]

        if init_method == 'xavier':
            limit = np.sqrt(6.0 / (fan_in + fan_out))
            data = np.random.uniform(-limit, limit, size=shape).astype(np.float32)
            return Tensor(data, requires_grad=True)
        elif init_method == 'kaiming':
            std = np.sqrt(2.0 / fan_in)
            data = np.random.normal(0.0, std, size=shape).astype(np.float32)
            return Tensor(data, requires_grad=True)
        else:
            raise(NotImplementedError('仅支持xavier初始化以及kaiming初始化！'))
```
支持更高维度的参数初始化。
- unsqueeze
```python
# tensor.py
...
    def unsqueeze(self, axis):
        if axis >= 0:
            target_shape = list(self.shape)[:axis] + [1] + list(self.shape)[axis:]
        elif axis == -1:
            target_shape = list(self.shape) + [1]
        else:
            target_shape = list(self.shape)[:axis+1] + [1] + list(self.shape)[axis+1:]
        return self.reshape(*target_shape)
```
修改unsqueeze对负数索引的逻辑，使其能够正确处理负数索引。
### 新增
- RMSNorm
```python
# nn.py
...
class RMSNorm(Module):
    def __init__(self, normalized_shape_len):
        self.gamma = Tensor.ones(normalized_shape_len)
        self.gamma.requires_grad = True

    def forward(self, x: Tensor):
        RMS = ((x ** 2).mean(-1, True) + 1e-9).sqrt()
        x_normalized = x / RMS
        return (self.gamma * x_normalized)
```
RMSNorm在现代大模型架构中通常用于替代LayerNorm并达到相近的效果并提高计算效率。RMSNorm不考虑均值的标准化，而只考虑缩放张量的尺度。RMS即均方根,数学上是二阶矩的平方根，直观地反映了张量的尺度。通过RMS可以有效地缩放张量的尺度，达到与LayerNorm相似的效果。此外，由于NMSNorm计算量小于LayerNorm，可以提高计算效率。
- swish、gelu、mish
```python
# tensor.py
...
    def tanh(self):
        exp_px = self.exp()
        exp_nx = (-self).exp()
        return (exp_px - exp_nx) / (exp_px + exp_nx)

    def sigmoid(self):
        return Tensor(1) / (1 + (-self).exp())

    def silu(self):
        sigmoid = self.sigmoid()
        return self * sigmoid

    swish = silu

    def gelu(self):
        return (1 / 2) * self * (1 + (Tensor(2 / np.pi).sqrt() * (self + 0.044715 * self ** 3)).tanh())

    def mish(self):
        return self * ((1 + self.exp()).log()).tanh()
```
三种现代使用的激活函数。三种激活函数都基于自门控的机制，即通过输入x的值得到一个门控值，再与x相乘并返回，通常可以表示为  
$$ f(x)=x*gate(x)$$  
事实上 ，relu可以视作是一个极端的自门控，门控值只能等于0或-1。
* silu  
全称是sigmoid linear unit，顾名思义，是将sigmoid作为门控函数。
* mish  
本质上是利用tanh作为门控函数。源于tanh与sigmoid的相似性，mish和silu也有很强的相似性。通常将mish视作silu的改进版。
* gelu  
本质上是将高斯分布的累积分布函数（即$Φ(x)=P(x<X),X~N(0,1)$)作为门控函数。特别地，由于$Φ(x)$不存在解析解，即不能由初等函数表示，因此再实际运用过程中都是用相似函数进行替代的。
三者的图像都十分相似，且都可以看作是平滑版的relu。这使得三者相较于relu都有更强的梯度回传能力，不易造成神经元死亡。现代大模型中通常默认使用gelu作为激活函数。
- DenseMOE
```python
# blocks.py
class Gate(nn.Module):
    def __init__(self, feature_in, num_experts):
        self.linear = nn.Linear(feature_in, num_experts)

    def forward(self, x):
        return self.linear(x).softmax(-1)


class DenseMOE(nn.Module):
    def __init__(self, feature_in, feature_out, num_experts):
        self.w = Tensor.parameter(num_experts, feature_in, feature_out)
        self.b = Tensor.parameter(num_experts, feature_out)
        self.gate = Gate(feature_in, num_experts)

    def forward(self, x):
        # x: b, s, i (feature_in)

        # 传入gate得到概率
        probs = self.gate(x)  # b, s, n (num_experts)

        # x并行传入experts
        # x: b, s, i, w: n, i, o (feature_out) -> b, s, n, o
        # 基于einsum,i维度在x和w都出现但在结果不出现。因此是沿i维度求和。
        # broadcast: x-> b,s,1,i,1  w-> 1,1,n,i,o 然后进行mul得到 b,s,n,i,o 的张量，再沿d维度求和降维
        x_uns = x.unsqueeze(2).unsqueeze(-1)  # b,s,1,i,1
        w_uns = self.w.unsqueeze(0).unsqueeze(0)  # 1,1,n,i,o
        experts_out_uns = x_uns * w_uns  # b,s,n,i,o
        experts_out = experts_out_uns.sum(-2)  # b,s,n,o
        experts_out = experts_out + self.b.unsqueeze(0).unsqueeze(0)  # 加偏置
        experts_out = experts_out.relu()

        # 加权求和
        # probs: b,s,n experts_out: b,s,n,o -> b,s,o
        probs_uns = probs.unsqueeze(-1)
        out_uns = experts_out * probs_uns
        return out_uns.sum(-2)  # b,s,o
```
实现了并行运算的DenseMOE。MOE（混合专家模型）的本质是基于一个门控网络(Gate)输出选中各个专家的概率值来筛选每个专家的输出(表现为加权和),理想状态下能得到多个精通不同领域的专家。  
此处DenseMOE的实现比较简易，每个expert本质上是一个线性层+relu。实际情况下，每个专家都应该是一个有充分学些能力的较小的FFN。  
为了实现x对不同专家的并行运算，这里利用了广播机制+mul的方式将einsum（爱因斯坦求和）算法直观地展示了出来。可以说，einsum的本质就是先将参与运算的矩阵广播到能够进行元素级乘法的形状，然后进行元素级乘法，然后再根据所需要的结果维度对不需要的维度进行sum操作进行降维。代码注释中已经详细地将einsum中每一步的张量形状写出，此处不再赘述。