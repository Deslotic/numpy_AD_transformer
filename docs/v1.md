# 项目技术文档
## 自动微分框架
### 写在前面
由于梯度下降算法分为前向传播和反向传播过程，为了避免混淆，后文中的父子节点概念都是基于前向传播过程的，即节点的创建顺序；上下游概念都是基于反向传播过程的，即梯度的反向传播顺序。
### 原理及其实现
#### 梯度下降算法
深度学习的基本原理。高维空间中，梯度代表函数上升最快的方向；相应的，逆梯度则是函数下降最快的方向。深度学习的核心就是减少损失，即衡量预测值(pred)和标签值(label)的差异。事实上，最经典的MSE损失函数的数学定义就是两个高维向量在每个维度中误差的平方和的平均值。缩小损失，即模型的学习过程。因此，为了缩小损失，沿着梯度的反方向进行下降即可收敛到高维曲面的极小值点（即使是局部极小值点）。
#### 实现方法
基于上述分析，我们的自动微分框架需要实现的功能有且只有一个：求梯度。基于链式求导法则，我们需要得到以下信息：
- 本地梯度  
我们需要每一个操作对应的梯度。在实际工作中，我们会记录每个张量经由的计算并对不同的计算定义相应的_backward方法（基于求导规则），在需要反向传播时，直接调用该方法即可对其下游节点的梯度进行修改。此外，由于链式法则，一个梯度可能来源于多个上游，因此需要对每个上游的梯度进行累加。
- 排序  
我们在反向传播时，下游的梯度始终来源于上游，因此需要对上下游进行排序。由于前向传播过程中的每个张量天然来源于各种父节点，因此可以将整个计算过程理解为一个有向图（计算图）。因此，我们在生成每个Tensor实例时都会记录其父节点，并在反向传播时进行拓扑排序，这样就可以得到一个子节点始终在父节点后的序列，从而保证反向传播过程中不会出现上游后于下游计算所导致的梯度尚未计算的问题。事实上，我们在反向传播时需要逆序遍历拓扑排序，因为在反向过程中父子节点是颠倒的。

#### 实现过程
样本在前向传播过程中每经过一次计算就会自动存储当前操作以及父节点，这个过程是生成计算图以及定义_backward方法的过程。_backward方法本质上是一个闭包，在定义时就会捕获对应的对象。反向传播时，对计算图中每个Tensor调用_backward方法即可反向地计算出参与过程中所有参数的梯度。反向传播完成后，优化器会对每个参数进行梯度下降。
### 深度代码解析
部分显而易见的代码将跳过。
#### tensor.py
支持自动微分的张量实现，本项目最难点。
- `__init__`
```python
class Tensor:    
    def __init__(self, data, requires_grad=False, _parents=(), _op=''):
        """

        :param data: ndarray或者可以转换为ndarray的数据类型，是Tensor实际的数据存放位置
        :param requires_grad: 是否需要梯度
        :param _parents: 该张量的来源，用于存储计算图以进行反向传播
        :param _op: 标识符，标志着该张量经由什么操作得来，不参与实际运算
        """

        # 确保data被转化为float32的ndarray
        if not isinstance(data, np.ndarray):
            try:
                data = np.array(data, dtype=np.float32)
            except ValueError:
                raise TypeError(f"Tensor 仅支持数值型数据, 收到: {type(data)}")

        if data.dtype != np.float32:
            data = data.astype(np.float32)

        self.data = data
        self.shape = self.data.shape
        self.requires_grad = requires_grad

        # 自动微分核心
        self.grad = None
        self._backward = lambda: None
        self._prev = set(_parents)  # 去重
        self._op = _op
```
初始化方法。`Tensor`的数值来源本质上是一个`ndarray`。本方法支持`ndarray`以及能够转换为`ndarray`的数据类型，并且会自动转换为`float32`单精度。`shape`属性继承于`data`，`requires_grad`标志是否需要梯度。  
grad参数用于存储梯度用以反向传播和梯度下降；`_backward`在此处是一个占位符，在前向传播过程中会生成；`_prev`用于存储父节点，使用集合是为了去重，如$ c = a + a + b$，不去重会导致梯度重复计算；`_op`为标识符，不实际参与运算。

- `backward`
```python
    def backward(self, gradient=None):
        """执行反向传播"""
        if not self.requires_grad:
            return  # 不需要梯度则直接返回

        if gradient is None:
            if self.data.size != 1:
                raise ValueError("对于非标量张量，需要指定gradient参数")
            gradient = np.array(1.0, dtype=np.float32)

        if not isinstance(gradient, np.ndarray):
            gradient = np.array(gradient, dtype=np.float32)

        if gradient.shape != self.shape:
            raise ValueError(f"张量与梯度形状不匹配")

        # 基于DFS的拓扑排序
        topo = []
        visited = set()

        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)

        build_topo(self)

        # 初始化所有梯度
        # 保证梯度累加的过程中不出错
        for v in topo:
            if v.requires_grad:
                if v.grad is None:
                    v.grad = np.zeros_like(v.data)

        # 反向传播
        self.grad = gradient

        for node in reversed(topo):
            node._backward()  # 这里的_backward()方法基于不同的运算有不同的求梯度方法
```
这个方法中包含四个部分。  
1、短路、异常处理。  
`backward`方法首先会判断当前张量是否需要梯度，如果不需要则直接返回；然后，判断`gradient`参数是否给出。通常情况下，我们都是对`loss`这一标量进行反向传播，因此不需要指定梯度，本方法会自动将其梯度置为`1`；如果不是对标量进行反向传播，则必须指定初始梯度，且初始梯度的形状要与当前张量相同（基于链式法则）  
2、拓扑排序，确定反向传播的顺序。  
这里是基于深度优先遍历递归实现的拓扑排序算法，是数据结构中必须掌握的方法。  
3、初始化梯度，即将计算图中所有节点的梯度初始化为`0`。  
由于同一个下游节点可能来源于多个上游节点，因此在梯度累加的过程中可能遇到梯度为`None`的情况，这里将其初始化为`0`可以避免这种情况。之所以在初始化张量时不直接将`grad`属性设为`0`是为了区分是否需要梯度。  
4、反向传播  
使用拓扑排序的逆序，依次调用每个节点的`_backward`方法。每个`_backward`方法会基于当前节点的梯度及其计算方式来修改其下游节点的梯度。

- `parameter`
```python
    @staticmethod
    def parameter(*shape, init_method='xavier'):
        """
        支持xavier初始化（transformer原始论文选择）、kaiming初始化（更适合relu激活函数）
        """

        # 支持1维的bias初始化以及2维的weight初始化
        fan_in = shape[0]
        fan_out = shape[-1] if len(shape) > 1 else 0

        if init_method == 'xavier':
            limit = np.sqrt(6.0 / (fan_in + fan_out))
            data = np.random.uniform(-limit, limit, size=shape).astype(np.float32)
            return Tensor(data, requires_grad=True)
        elif init_method == 'kaiming':
            std = np.sqrt(2.0 / fan_in)
            data = np.random.normal(0.0, std, size=shape).astype(np.float32)
            return Tensor(data, requires_grad=True)
        else:
            raise(NotImplementedError('仅支持xavier初始化以及kaiming初始化！'))
```
本质上是一个构造方法，用于创建参数对象。事实上，在pytorch中，参数对象是被包装为`Parameter`实例的，在这里，我们只返回一个需要梯度的、经过初始化的`Tensor`实例，其本质是相同的。即一个可训练（即需求梯度的）、经过初始化的、特定形状的张量。此处的`parameter`方法支持2维的参数构造，足以支撑原始`transformer`的架构，因为无论是`MHA`、`FFN`，其本质上都是线性层。此外，这里给出了两种初始化方法，均直接根据公式实现。其中`xavier`初始化是`transformer`论文的原始选择、`kaiming`初始化适配`ReLU`激活函数。
- `_unbroadcast`
```python
    @staticmethod
    def _unbroadcast(target_shape, grad):
        """
        反向传播时处理广播的辅助函数。
        :param target_shape: 目标形状
        :param grad: 梯度
        :return: 缩减后的梯度
        """
        # 撤销不存在的维度的广播 eg (3) -> (3,3)
        while len(grad.shape) > len(target_shape):
            grad = grad.sum(axis=0)

        # 撤销维度为1的广播 eg (1,3) -> (3,3)
        for i, (dim_target, dim_grad) in enumerate(zip(target_shape, grad.shape)):
            if dim_target == 1 and dim_grad > 1:
                grad = grad.sum(axis=i, keepdims=True)

        # 标量情况，如果target_shape是() (标量)，而grad是(1,)
        if grad.shape != target_shape:
            grad = grad.sum()
        return grad
```
是反向传播中相当重要的方法。`numpy`的广播机制会在前向传播过程中进行广播（本质上是将待广播的数组在更高维度上进行“复制”），因此子节点的形状是广播后的形状。因此，在反向过程中，由于上游节点经过广播，其形状与下游节点不匹配，无法计算梯度，因此需要这个方法来撤销广播。具体到做法上，由于链式求导法则，下游节点的梯度是所有上游节点到下游节点链路中梯度的总和，因此直接将上游节点沿高纬度求和即可。  
这里的`_unbroadcast`方法就是自适应撤销广播的过程：首先沿着高纬度（`axis=0`）依次调用`sum`方法降维，使形状的长度相同；然后遍历每一个维度，同样进行求和操作，设定`keepdim=True`。这两步完成了对广播的撤销。`numpy`中，广播机制只针对不存在的维度或维度为1的维度(即 `(3,) -> (3,3); (1,3) -> (3,3)`)，因此这两个操作即可撤销广播。经过这两步操作后，只有一种情况会导致形状不匹配，即目标形状是标量，这种情况下再调用一次`sum`即可。理论上有且仅有这三种情况，因此不需要进行额外的判断。

- 运算类  

在后面的代码中，可以注意到我们会对`other`首先进行张量的包装，这是为了在存储计算图时明确存储参与计算的所有张量。对于一些我们不关注梯度的值，我们可能并不会在外部就显式的包装为张量（因为我们并不在意）；因此在内部，即使我们不需要某些张量的梯度，我们仍然需要保存其数值，因为其数值很可能参与了其他张量的梯度计算。

- `_backward `

在解析`Tensor`的基础数学运算前，我们需要先详细说明`_backward`的作用模式。在此处以及后面所有的`_backward`方法中，所记录的`self`、`other`等实际上是捕获了对应的对象，然后再将这个嵌套函数(一个闭包)赋值给了`out._backward`。因此，在调用`out._backward()`时，实际上是根据`out`当前的`grad`计算出其下游节点的`grad`，然后进行的累加操作。即：每个节点的`_backward`方法实际上影响的是其下游节点的梯度，而本节点的梯度在调用`_backward`方法之前就已经计算完毕了（基于拓扑排序）。也因此，最上游的节点需要手动指定梯度（或默认梯度为1），因为其不存在上游节点，而作为起始节点，其对自身的梯度理应是1。此外，由于链式求导法则，我们每一个`_backward`所求的梯度都应该是上游梯度与本地梯度的乘积。
- `__add__`
```python
    def __add__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(
            data=self.data + other.data,
            _parents=(self, other),
            _op='+'
        )
        out.requires_grad = self.requires_grad or other.requires_grad

        def _backward():
            if self.requires_grad:
                self.grad += self._unbroadcast(self.shape, out.grad)
            if other.requires_grad:
                other.grad += other._unbroadcast(other.shape, out.grad)

        out._backward = _backward
        return out
```
加法。记录父节点和标识符，并且设定`_backward`方法。下面的解析中将不解析`out`张量，而是专注于介绍`_backward`函数的计算过程。对于张量运算

$$C = A + B$$

$$\frac{dL}{dA} = \frac{dL}{dC} * \frac{dC}{dA}$$

前者为 `out.grad`，后者为 1，因此

$$\frac{dL}{dA}=out.grad*1=out.grad$$

B的梯度类似。同时为了撤销可能的广播，调用`_unbroadcast`方法。
- `__mul__`
```python
    def __mul__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(
            data=self.data * other.data,
            _parents=(self, other),
            _op='*'
        )
        out.requires_grad = self.requires_grad or other.requires_grad

        def _backward():
            if self.requires_grad:
                self.grad += self._unbroadcast(self.shape, other.data * out.grad)
            if other.requires_grad:
                other.grad += other._unbroadcast(other.shape, self.data * out.grad)

        out._backward = _backward
        return out
```
数乘乘法/元素级乘法。对于张量计算

$$C = A * B$$

有

$$C[i,j] = A[i,j] * B[i, j]$$

则

$$\frac{dL}{dA} = \frac{dL}{dC} * \frac{dC}{dA}$$

$$\frac{dC}{dA}=B$$

因此

$$grad_A=out.grad * B$$

对B同理。此处为元素级乘法，满足交换律，无需考虑顺序。同样地，为了避免可能的广播，需要调用`_unbroadcast`方法进行撤销。
- `__truediv__`
```python
    def __truediv__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(
            data=self.data / other.data,
            _parents=(self, other),
            _op='/'
        )
        out.requires_grad = self.requires_grad or other.requires_grad

        def _backward():
            if self.requires_grad:
                self.grad += self._unbroadcast(self.shape, out.grad / other.data)
            if other.requires_grad:
                other.grad += other._unbroadcast(other.shape, - (self.data * out.grad / other.data ** 2))

        out._backward = _backward
        return out
```
元素级除法，梯度计算过程十分简单。这里我们说明为什么要选择要直接从除法出发，而不是用下列形式模拟除法：
```python
    def __truediv__(self, other):
		return self * (other ** -1)
```
二者在数学上是完全等价的，但在`python`中二者有一个细微差别：浮点数`0.0`可以做除数，当` 0.0/0.0` 时得到` nan`，其余得到`+inf`或`-inf`。而使用乘方进行模拟无法达到这种效果，因为`0`无法做负数幂的底数（即使是`float`也不行）。在计算损失时，如果传入空批次，我们希望`Loss`的梯度被计算为`nan`并继续反向传播使参与计算的所有参数的梯度都被计算为`nan`以警告用户训练中出现问题，而这种操作刚好可以利用` 0.0/0.0 `返回`nan`来静默地实现。事实上,这正是`pytorch`的做法。
- `__matmul__`
```python
    def __matmul__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(
            data=self.data @ other.data,
            _parents=(self, other),
            _op='@'
        )
        out.requires_grad = self.requires_grad or other.requires_grad

        def _backward():
            if self.requires_grad:
                other_T_axes = list(range(len(other.shape)))
                # 转置后两个维度
                if len(other_T_axes) >= 2:
                    other_T_axes[-1], other_T_axes[-2] = other_T_axes[-2], other_T_axes[-1]

                self.grad += self._unbroadcast(
                    self.shape,
                    out.grad @ np.transpose(other.data, other_T_axes)
                )

            if other.requires_grad:
                self_T_axes = list(range(len(self.shape)))
                if len(self_T_axes) >= 2:
                    self_T_axes[-1], self_T_axes[-2] = self_T_axes[-2], self_T_axes[-1]

                other.grad += other._unbroadcast(
                    other.shape,
                    np.transpose(self.data, self_T_axes) @ out.grad
                )

        out._backward = _backward
        return out
```
矩阵乘法，梯度下降算法中最重要的一点，因为线性层的本质就是矩阵乘法。首先，矩阵和矩阵乘法不构成阿贝尔群，因此顺序需要严格遵守。对于张量计算

$$C = A @ B$$

有

$$C[i, j] = \sum_{k=0}^{n-1} A[i, k] * B[k ,j]$$

由于对A中的任一元素 $A[i,j]$，其只会参与 $C$ 中第 i 行的计算，因此

$$\frac{dL}{dA[i,j]}=\sum_{k=0}^{n-1} \frac{dL}{dC[i,k]} * \frac{dC[i,k]}{dA[i,j]}$$

而对于 $C$ 中第 $i$ 行的元素 $C[i,k]$，其中只有一项与 $A[i,j]$ 相关，其系数为 $B[j,k]$，也即 $B^T[k,j]$。带入上式，我们有

$$\frac{dL}{dA[i,j]}=\sum_{k=0}^{n-1} \frac{dL}{dC[i,k]} * B^T[k,j]$$

如果我们把 $\frac{dL}{dC}$ 和 $B^T$ 看成两个矩阵（事实上就是两个矩阵），则上式正是矩阵乘法中计算 i，j 位置元素的方法。因此，我们得到

$$grad_A = grad_C * B^T$$

对于B，我们做类似的操作，只需要注意交换律不生效，即可得到

$$grad_B=A^T * grad_C$$

综上所述，我们推导出了矩阵乘法的梯度表示。
- `__pow__`
```python
    def __pow__(self, other):
        assert isinstance(other, (int, float)), "只支持标量幂"
        out = Tensor(
            data=self.data ** other,
            _parents=(self,),
            _op=f'**{other}'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad: self.grad += (other * (self.data ** (other - 1))) * out.grad

        out._backward = _backward
        return out
```
标量的幂运算，即元素级的幂运算，则对于

$$B=A^2$$

对其中每个元素求导数即可，简单的标量运算。
- 其余运算以及反运算符
```python
    def __neg__(self):
        return self * -1.0

    def __sub__(self, other):
        return self + (other * -1.0)

    def __radd__(self, other):
        return self + other

    def __rmul__(self, other):
        return self * other

    def __rsub__(self, other):
        return Tensor(other) - self
```
这几个方法本质上是调用上面的基础运算，因此不需要额外指定_backward方法，这几个方法在底层会调用上面几个方法并存储计算符。这里需要说明反运算符的概念。首先，反运算符是基于阿贝尔群的，因为这样才满足交换律。显然，矩阵加法以及元素级乘法都是阿贝尔的，因此可以使用反运算符。  
下面以加法举例：在实际调用过程中，如果遇到了"+"，python会首先尝试用调用前面元素的add方法，如果没有则尝试调用后面函数的`__radd__`方法。反运算符可以实现整数、浮点数放在张量前面的正常运算。

- `relu`
```python
    def relu(self):
        out = Tensor(
            data=np.maximum(0, self.data),
            _parents=(self,),
            _op='ReLU'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad: self.grad += (self.data > 0).astype(np.float32) * out.grad

        out._backward = _backward
        return out
```
截断数值＜0部分的梯度。
- `exp`
```python
    def exp(self):
        out = Tensor(
            data=np.exp(self.data),
            _parents=(self,),
            _op='exp'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad: self.grad += out.data * out.grad

        out._backward = _backward
        return out
```
即e的指数函数，其本地梯度是`out`，因此直接将`out`与上游梯度相乘即可。
- `log`
```python
    def log(self):
        out = Tensor(
            data=np.log(self.data),
            _parents=(self,),
            _op='log'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad: self.grad += (1.0 / self.data) * out.grad

        out._backward = _backward
        return out
```
对数函数，其梯度是其倒数，因此倒数与上游梯度相乘。
- `sqrt`
```python
    def sqrt(self):
        out = Tensor(
            data=np.sqrt(self.data),
            _parents=(self,),
            _op='sqrt'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad: self.grad += 1 / (2 * out.data + 1e-9) * out.grad

        out._backward = _backward
        return out
```
从数学上可以直接复用`__pow__`，但是单独写一个`sqrt`方法有3方面考虑：

1、代码的易用性，不用手动实现0.5次方, 直接使用`sqrt`方法即可。

2、数值稳定性，分母加上极小量防止除零错误。

3、直接使用`out.data`,不用手动计算0.5次方，减少计算量,提高运行效率。

- `sum`、`mean`
```python
    def sum(self, axis=None, keepdims=False):
        out = Tensor(
            data=np.sum(self.data, axis=axis, keepdims=keepdims),
            _parents=(self,),
            _op='sum'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad:
                grad_shape = self.shape
                if axis is not None and not keepdims:
                    grad = np.expand_dims(out.grad, axis)
                else:
                    grad = out.grad
                self.grad += np.broadcast_to(grad, grad_shape)

        out._backward = _backward
        return out

    def mean(self, axis=None, keepdims=False):
        N = np.prod(self.data.shape) if axis is None else self.data.shape[axis]
        out = self.sum(axis=axis, keepdims=keepdims) / N
        out._op = 'mean'
        return out
```
对于`sum`方法，$y=x_1+x_2+x_3$，显然有$y$对每一个分量的偏导数都等于$1$，因此对`sum`进行反向传播实际上是将上游梯度广播到`sum`之前的维度上。此处的`_backward`主要是在根据`sum`的`axis`和`keepdims`参数来调整广播后的形状。

`if axis is not None and not keepdims`判断指的是如果设定了轴且保持没有保持维度，需要在降维的维度重新升维以保证广播能够正常进行。如 `(2,3)` 在 `axis=1` 处求`sum`，则形状为 `(2,) `的上游梯度无法直接广播为` (2,3)`,而`(2,1)`可以。  
`mean`方法直接复用`sum`以及`__truediv__`即可。

- `max`
```python
    def max(self, axis=None, keepdims=False):
        out = Tensor(
            data=np.max(self.data, axis=axis, keepdims=keepdims),
            _parents=(self,),
            _op='max'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad:
                max_val = out.data
                if axis is not None and not keepdims:
                    max_val = np.expand_dims(max_val, axis)

                mask = (self.data == max_val).astype(np.float32)
                mask_sum = mask.sum(axis=axis, keepdims=True)  # 求mask在对应轴上的和，即每个轴上有多少个最大值
                mask = mask / np.where(mask_sum == 0, 1.0, mask_sum)  # 平分梯度，使用where避免除以0

                grad = out.grad
                if axis is not None and not keepdims:
                    grad = np.expand_dims(grad, axis)

                self.grad += mask * grad

        out._backward = _backward
        return out
```
此处`_backward`方法实现了三个功能。  
1、获取`mask`，即`self.data`中最大值才具有梯度，而其余位置梯度为0。  
2、处理广播情况，与`sum`完全相同。  
3、多个最大值时，平分梯度。

- `shaping`
```python
    def reshape(self, *shape):
        out = Tensor(
            data=self.data.reshape(*shape),
            _parents=(self,),
            _op='reshape'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad: self.grad += out.grad.reshape(self.data.shape)

        out._backward = _backward
        return out

    def transpose(self, *axes):
        out = Tensor(
            data=np.transpose(self.data, axes),
            _parents=(self,),
            _op='transpose'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad:
                inv_axes = np.argsort(axes)
                self.grad += np.transpose(out.grad, inv_axes)

        out._backward = _backward
        return out
```
将梯度在反向传播时以逆的方式塑形即可。特别地，可以证明置换P的逆置换是`argsort(P)`。对于置换$P$，其值是索引，因此排序后的P有

$$P[argsort(P)] = \{0, 1, 2 ... n-1\}$$

因此对于

$$Q=argsort(P)$$

有 $P(Q(k))=k$，即PQ互逆。
- `__getitem__`
```python
    def __getitem__(self, item):
        out = Tensor(
            data=self.data[item],
            _parents=(self,),
            _op='getitem'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad:
                np.add.at(self.grad, item, out.grad)

        out._backward = _backward
        return out
```
这里需要注意的是使用了`np.add.at`方法来规避链式法则中对同一个内存地址进行多次`+=`操作所带来的计算不正确的问题。
- `concatenate`、`cat`
```python
    @staticmethod
    def concatenate(tensors, axis=0):
        data = np.concatenate([t.data for t in tensors], axis=axis)
        out = Tensor(
            data=data,
            _parents=set(tensors),
            _op='concat'
        )
        out.requires_grad = any(t.requires_grad for t in tensors)

        def _backward():
            indices = np.cumsum([t.shape[axis] for t in tensors])
            slices = np.split(out.grad, indices[:-1], axis=axis)
            for t, s_grad in zip(tensors, slices):
                if t.requires_grad:
                    t.grad += s_grad

        out._backward = _backward
        return out
    
    @staticmethod
    def cat(tensors, axis=0):
        return Tensor.concatenate(tensors, axis)
```
`_backward`实现了三步操作：  

1. 根据张量列表的形状将拼接后的各个张量的索引找出来。

2. 根据索引将梯度分割为原始张量列表的对应的梯度张量列表。

3. 向下游节点分发对应的梯度张量。

   `cat`是更符合`pytorch`风格的命名。
- `masked_fill`
```python
    def masked_fill(self, mask, value):
        if not isinstance(mask, np.ndarray):
            raise TypeError("mask 必须是 numpy 数组")

        value = float(value)
        out = Tensor(
            data=np.where(mask, value, self.data),
            _parents=(self,),
            _op='masked_fill'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad:
                self.grad += out.grad * (1.0 - mask.astype(np.float32))

        out._backward = _backward
        return out
```
在经过`masked_fill`后的元素将不再与父节点有关，因此反向传播时将没有被`mask`的位置进行梯度的反向传播。
- `logexpsum`和`logsoftmax`以及`softmax`
```python
    def logsumexp(self, axis=-1, keepdims=False):
        max_val = self.max(axis=axis, keepdims=True).detach()
        x_shifted = self - max_val
        exp_shifted = x_shifted.exp()
        sum_exp_shifted = exp_shifted.sum(axis=axis, keepdims=keepdims)
        log_sum_exp = sum_exp_shifted.log() + max_val.reshape(sum_exp_shifted.shape)
        log_sum_exp._op = 'logsumexp'
        return log_sum_exp

    def logsoftmax(self, axis=-1):
        lse = self.logsumexp(axis=axis, keepdims=True)
        out = self - lse
        out._op = 'logsoftmax'
        return out
    
    def softmax(self, axis=-1):
        return self.logsoftmax(axis=axis).exp()

```
因为都是基于已有计算的组合，因此不需要另外加入计算图。这里讨论的是`logsoftmax`对于`softmax`的数值稳定优势。首先，`logsoftmax`在数学上只是将`softmax`后的概率求自然对数。由于对数是单调递增的，`logsoftmax`不会影响`softmax`反映概率大小的能力。因此使用`logsoftmax`是理论可行的。现在我们考虑`logsoftmax`的数值稳定性优势。首先，`softmax`伴随着大量的$e$的次幂运算，面对极端数据时极易发生上下溢，而$log$可以将幂的积转换为和，因此可以极大地提升数值稳定性。具体的讲，对于

$$log(\sum{e^x})=log(\sum{e^{(x-x_{max}+x_{max})}})=x_{max} +log(\sum{e^{(x-x_{max})}})$$

而 $x-x_{max}$ 始终是$<0$的，因此不会发生上溢。此外，计算 $log(softmax(x))$ 时如果直接计算，容易发生下溢导致 $log(0)$，我们同样可以将分子部分提出$log$，则

$$log(softmax(x)) = x - logexpsum(X)$$

`softmax`在这里是基于`logsoftmax`和`exp`实现的,在数学上完全可行但是实际上可能有数据溢出问题(直接计算softmax也会有数值溢出问题)。后续项目迭代的过程中会尝试修改`softmax`的实现。

- `take_along_axis`
```python
    def take_along_axis(self, indices, axis=-1):
        if not isinstance(indices, np.ndarray):
            raise TypeError("indices 必须是 numpy 数组")

        out = Tensor(
            data=np.take_along_axis(self.data, indices, axis=axis),
            _parents=(self,),
            _op='take_along_axis'
        )
        out.requires_grad = self.requires_grad

        def _backward():
            if self.requires_grad:
                grad_in = np.zeros_like(self.data)
                np.put_along_axis(grad_in, indices, out.grad, axis=axis)
                self.grad += grad_in

        out._backward = _backward
        return out
```
这是交叉熵损失中`softmax`后`NLLLoss`的主要操作，其作用是沿特定轴将指定`axis`的元素取出，即从`softmax`输出的`logits`中取出标签所对应的那一个。因此其逆操作应该为：将对应位置的梯度分发到`logits`形状的张量中。`put_along_axis`方法就可以实现这个操作。其数学意义在于：只有正确的位置的`logits`才会进行梯度下降，而其余位置的梯度都为0。
#### nn.py
实现了神经网络中部分基本模块
- `Module`基类
```python
class Module:
    def parameters(self):
        params = []
        for name, attr in self.__dict__.items():
            if isinstance(attr, Tensor) and attr.requires_grad:
                params.append(attr)
            elif isinstance(attr, Module):
                params.extend(attr.parameters())
            elif isinstance(attr, list):  # 用于 ModuleList
                for item in attr:
                    if isinstance(item, Module):
                        params.extend(item.parameters())
        # 去重
        return list(dict.fromkeys(params))

    def zero_grad(self):
        for p in self.parameters():
            p.zero_grad()

    def forward(self, *args, **kwargs):
        pass

    def train(self, is_train=True):
        if hasattr(self, 'is_train'):
            self.is_train = is_train
        for name, attr in self.__dict__.items():
            if isinstance(attr, Module):
                attr.train(is_train)
        return self

    def eval(self):
        return self.train(False)

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)
```
主要实现的是参数相关的方法。`parameters`方法递归地找出当前对象及其子对象的所有可训练参数并去重。这个方法首先遍历当前对象的所有属性，并依次进行判断：如果某属性是可训练参数，则直接加入列表；如果某属性也是`Module`实例，则递归调用`parameters`方法返回参数列表并`extend`到当前列表；如果某属性是`list`对象，则遍历并添加参数。这里没有额外封装`ModuleList`类，因此需要用额外判断列表内元素的数据类型。  
`zero_grad`简单地迭代调用参数清除梯度的方法。  
`train`、`eval`方法模拟了`pytorch`中的同种方法。在本项目中，暂时只影响`dropout`层。

- `Linear`
```python
class Linear(Module):
    def __init__(self, in_features, out_features, bias=True):
        self.W = Tensor.parameter(in_features, out_features)
        self.b = Tensor.parameter(out_features) if bias else None

    def forward(self, x):
        out = x @ self.W
        if self.b is not None:
            out = out + self.b
        return out
```
线性层/全连接层，本质上就是矩阵乘法。这里的bias相加过程需要广播，这也是`Tensor.__add__ `方法需要调用`_unbroadcast`方法的原因之一。
- `Embedding`
```python
class Embedding(Module):
    def __init__(self, num_embeddings, embedding_dim):
        self.weight = Tensor.parameter(num_embeddings, embedding_dim)

    def forward(self, indices):
        # indices 是一个 numpy int 数组
        return self.weight[indices]
```
这里的`Embedding`实现方式是使用`__get_item__`方法实现的，因此在`Tensor`对应方法需要调用`np.add.at`防止对同一个内存的`+=`出错。  
此外，我们还可以将`Embedding`层视作一个不带偏置的线性层，输入是经过`one_hot`的张量（这里我们将`one_hot`放在内部）。`one_hot`张量的矩阵乘法实际上就是在进行`__get_item`__操作。代码如下：

```python
class Embedding2(Module):
    def __init__(self, num_embeddings, embedding_dim):
        self.num_embeddings = num_embeddings
        self.weight = Tensor.parameter(num_embeddings, embedding_dim)

    def forward(self, indices):
        return self._one_hot(indices) @ self.weight

    def _one_hot(self, indices):
        ret = np.zeros((len(indices), self.num_embeddings))
        for i,j in enumerate(indices):
            ret[i,j] = 1
        return ret
```
二者在数学上完全等价，但后者由于创建了一个可能十分庞大的稀疏矩阵，因此在运行效率上远不如前者。
- `LayerNorm`
```python
class LayerNorm(Module):
    def __init__(self, normalized_shape_len,):
        self.gamma = Tensor.ones(normalized_shape_len)
        self.beta = Tensor.zeros(normalized_shape_len)
        self.gamma.requires_grad = True
        self.beta.requires_grad = True

    def forward(self, x):
        # x: (B, L, D) -> 归一化最后一个维度 (D)
        mean = x.mean(axis=-1, keepdims=True)
        var = ((x - mean) ** 2).mean(axis=-1, keepdims=True)

        x_normalized = (x - mean) / (var.sqrt() + 1e-9)

        # 广播 gamma 和 beta
        return (self.gamma * x_normalized) + self.beta
```
将数值进行归一化。本质上是将随机变量的分布缩放为$N(0,1)$，有助于保持数值稳定。此外，为了学习特征，`LayerNorm`引入了两个可训练的参数`gamma`、`beta`，这本质上与归一化过程无关。
- `ReLU`
```python
class ReLU(Module):
    def forward(self, x):
        return x.relu()
```
本质上是调用`Tensor`的`relu`方法，这里写成类是为了对应`pytorch`的设计哲学。
- `Dropout`
```python
class Dropout(Module):
    def __init__(self, p=0.1):
        self.p = p
        self.is_training = True  # 模拟 train/eval 模式

    def forward(self, x):
        if not self.is_training or self.p == 0:
            return x

        # 创建一个 numpy mask
        mask = (np.random.rand(*x.shape) > self.p).astype(np.float32)
        # 缩放，尺度补偿
        out = (x * mask) * (1.0 / (1.0 - self.p))

        def _backward():
            if x.requires_grad:
                x.grad += out.grad * mask * (1.0 / (1.0 - self.p))

        out._prev = {x}
        out._op = 'dropout'
        out._backward = _backward
        out.requires_grad = x.requires_grad

        return out
```
本质上是对张量中的每一个元素以某一概率置`0`，但是在实际操作过程中，由于部分元素被丢弃导致张量的总尺度减小了，因此需要对其余没有抑制的值进行缩放以保证尺度的一致性。`_backward`实现的方法实际上与`masked_fill`完全类似，即只传递没有被修改的位置的梯度。同时，在模型的推理过程中`Dropout`层不生效，这里使用i`s_training`参数进行控制。  

这里再额外说明一下`Dropout`、`BatchNrom`以及`LayerNorm`的屏蔽机制，为什么前面两者在训练时需要被屏蔽而后者不需要？  

- 首先，`Dropout`在推理时如果不屏蔽将会使相同的数据得到不同的输出，这是不可接受的，因此需要屏蔽。 

- `BatchNorm`被屏蔽的原因是：如果同一个样本被放入不同的`batch`，则参与计算的均值和方差会受`batch`其他样本，这也是不可接受的。因此，在推理时，`BatchNorm`会使用训练过程中学习到的全局均值和全局方差进行运算。因此，`BatchNorm`严格意义上不是被屏蔽，而是不额外计算均值和方差，这与Dropout的屏蔽有本质区别。
- 而`LayerNorm`计算均值和方差的尺度不同，是对词嵌入维度的尺度进行计算的。因此，在训练和推理时所计算的值都只依赖于这个样本，不会出现不稳定的情况，因此不需要屏蔽。

#### loss.py
##### CrossEntropyLoss  
在讨论交叉熵损失函数之前，我们先讨论`softmax`以及`crossentropy`的数学原理。
###### softmax

$$softmax(x_i)=\frac{e^{x_i}}{\sum{e^x}}$$

`softmax`有几个天然的特性：  

1.  输出值满足概率形式，即取值范围在(0,1)区间
2.  输出值和为1

3. 保持`softmax`前的大小关系  

这三点都基于非常简单的数学原理，此处略过。

此外，`softmax`对较大的值很敏感，较大的`logits`在经过`softmax`后会相较于线性的缩放更快收敛到`target`上。但是这个特性也使`softmax`有一个饱和区间，当`logits`较大时，`softmax`输出概率退化为`one_hot`,本地梯度可能消失。  
我们现在先计算一下`softmax`的本地梯度。设

$$y_i=softmax(x_i))=\frac{e^{x_i}}{\sum{e^x}}$$

考虑

$$\frac{dy_i}{dx_i}=\frac{d(\frac{e^{x_i}}{\sum{e^x}})}{dx_i}=\frac{(e^{x_i})^{'}*\sum{e^x}-e^{x_i}*(\sum{e^x})^{'}}{(\sum{e^x})^2}$$

观察等式中两个导数，前者为`exp`求导，等于其本身；后者为多项式和，其中只有一项与求导变量相关，因此导数与前者相同，整理得

$$\frac{dy_i}{dx_i}=\frac{e^{x_i}*\sum{e^x}-e^{x_i}*e^{x_i}}{(\sum{e^x})^2}=\frac{e^{x_i}}{\sum{e^x}}-(\frac{e^{x_i}}{\sum{e^x}})^2=y_i-{y_i}^2=y_i*(1-y_i)$$

因此当`softmax`进入饱和区时，输出会退化成`one_hot`张量，对应的本地梯度为`0`，导致梯度消失。为了避免这点，我们在使用`softmax`时有两种做法：  
1、配合交叉熵损失函数，梯度计算在后文；  
2、在传入`softmax`前先进行缩放，避免进入`softmax`饱和区（这也正是缩放点积注意力中为什么要除以`sqrt(d_k)`的数学原因。  
此外，当 $i\neq j$ 时，也存在梯度，有

$$\frac{dy_j}{dx_i}=\frac{-e^{x_j}*e^{x_i}}{(\sum e^x)^2}=-y_j*y_i$$

###### crossentropy

$$L=-\sum t_ilog(y_i)$$

此处的 $y_i$ 即`softmax`输出概率，$t_i$ 为真实标签，其反映的是两个概率分布 ${t_i}$、${y_i}$ 的差异度。考虑本地梯度

$$\frac{dL}{dy_i}=-\frac{t_i}{y_i}$$

因此，交叉熵损失函数的梯度

$$\frac{dL}{dx_i}=\sum_{j}\frac{dL}{dy_j}*\frac{dy_j}{dx_i}=t_iy_i-t_i + \sum_{j\neq i}t_jy_i=y_i\sum_j t_j -t_i$$

由于 $t_j$ 是`one_hot`向量，则 $\sum_j t_j=1$，因此有

$$\frac{dL}{dx_i}=y_i-t_i$$

可以看到，在经过严格推导后，交叉熵损失函数的梯度就是`softmax`输出概率与标签之差，这无疑是符合直觉的，且不再会出现梯度消失问题。  
###### CrossEntropyLoss
```python
class CrossEntropyLoss(Module):
    def __init__(self, ignore_index=-1):
        self.ignore_index = ignore_index

    def forward(self, logits, targets):
        # 计算 LogSoftmax,输出对数概率
        log_probs = logits.logsoftmax(axis=-1)  # (B, L, V)

        # 将target升维以匹配take_along_axis方法
        target_axis = np.expand_dims(targets, axis=-1)

        # 取nll并降维
        nll = -log_probs.take_along_axis(target_axis, axis=-1).squeeze()

        # 屏蔽pad_id并应用mask
        mask = (targets != self.ignore_index).astype(np.float32)
        loss = (nll * mask).sum()

        # 缩放损失
        num_active = mask.sum()
        # 如果num_active=0，即batch为空批次，会直接得到一个值为nan的Tensor，在反向传播后会将所有梯度都计算为nan，以告知训练出错
        mean_loss = loss / num_active
        mean_loss._op = 'CrossEntropyLoss'

        return mean_loss
```
然而，在实际的实现中，我们似乎不是按照`softmax`$+$`crossentropy`的方式来计算损失的。事实上，我们可以观察到，`softmax`输出的概率并不会直接进行计算，而总是在对数中（这是由于信息量的概念）。恰好，为了避免`softmax`可能带来的数值上溢问题，我们刚好可以利用对数运算的降次特性来规避这个问题（在Tensor部分详细介绍过）。因此，`softmax`$+$`crossentropy`被我们重构为`logsoftmax`$+$`nll`，这二者在数学上是完全等价的。`NLLLoss`全称为负对数似然损失，经过`logsoftmax`输出的对数概率取值范围为 $(-\inf,0)$，因此要让`NLLLoss`中正确标签对应的对数概率尽可能接近0，即原始概率尽可能接近1。同时,`NLLLoss`要尽量小的特性也完全符合Loss的梯度下降原理,将需要尽可能大的概率问题转换为需要尽可能小的损失问题。
#### optim.py
- `SGD`
```python
class SGD:
    def __init__(self, params, lr=0.01):
        self.params = params
        self.lr = lr

    def step(self):
        for p in self.params:
            if p.requires_grad:
                p.data -= self.lr * p.grad

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.zero_grad()
```
事实上，SGD优化器与SGD(随机梯度下降算法)没有本质的关系。SGD优化器只是访问模型中所有的参数并按照设定的学习率进行梯度下降而已。真正实现SGD的部分来自我们训练时选择的策略。假设我们设置`batch_size=1`，则每次取一个随机的样本反向传播并进行梯度下降，就是SGD算法的本质。  

至此，自动微分框架解析完毕。
## transformer
本项目的`transformer`实现基本上完全基于原始论文，因此不会对代码有过于详细的说明，仅说明部分数学原理。
### blocks.py
特别注意，为了保证与`pytorch`代码哲学的一致性，这里的`nn.Module`源于手写自动微分框架的nn.py文件。
- `ScaledDotProductAttention`
```python
class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k, dropout_p=0.1):
        self.d_k = Tensor(d_k)
        self.softmax = lambda x: x.softmax(axis=-1)
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, Q, K, V, mask=None):
        scores = (Q @ K.transpose(0, 1, 3, 2)) / self.d_k.sqrt()
        if mask is not None:
            scores = scores.masked_fill(mask, -1e9)
        attn_weights = self.dropout(self.softmax(scores))
        return attn_weights @ V
```
缩放点积注意力。之所以要进行缩放是为了防止`softmax`饱和所导致的梯度消失，具体在交叉熵损失函数部分有详细推导。  
将`mask`的部分填充为`-1e9`的原因是经过`softmax`输出0，不会对概率分布产生影响。

- `MultiHeadAttention`
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout_p=0.1):
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)

        self.attn = ScaledDotProductAttention(self.d_k, dropout_p)

        self.out_proj = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout_p)
        self.ln = nn.LayerNorm(d_model)

    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]

        Q = self.q_proj(query)
        K = self.k_proj(key)
        V = self.v_proj(value)

        Q = Q.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        K = K.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        V = V.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(0, 2, 1, 3)

        attn_out = self.attn(Q, K, V, np.expand_dims(mask, 1))  # mask在head维度升维（用于广播）以适配多头注意力
        attn_out = attn_out.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)

        output = self.out_proj(attn_out)
        res = self.dropout(output) + query
        return self.ln(res)
```
多头注意力机制的经典实现。此处的`mask`在`head`维度进行升维，是因为不希望外部知道内部的结构（是否使用了多头注意力），期望收到形状为`(batch_size,len_q,len_k)`或可广播形状的`mask`。
- `FFN`
```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout_p=0.1):
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_p)
        self.ln = nn.LayerNorm(d_model)

    def forward(self, x):
        out = self.dropout(self.fc2(self.relu(self.fc1(x))))
        return self.ln(out + x)
```
经典实现。
- `EncoderLayer`
```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout_p=0.1):
        self.mha = MultiHeadAttention(d_model, num_heads, dropout_p)
        self.ffn = FeedForward(d_model, d_ff, dropout_p)

    def forward(self, x, src_mask=None):
        return self.ffn(self.mha(x, x, x, src_mask))
```
经典实现。
- `PositionEncoding`
```python
class PositionEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        pe = np.zeros((max_len, d_model))
        pos = np.expand_dims(np.arange(0, max_len), -1)
        div_term = np.pow(np.array(10000), np.arange(0, d_model, 2) / d_model)
        inner = pos / div_term
        pe[:, ::2] = np.sin(inner)
        pe[:, 1::2] = np.cos(inner)
        self.pe = Tensor(pe).unsqueeze(0)

    def forward(self, x):
        return x + self.pe[:, :x.shape[1]]
```
位置编码层的经典实现。之所以使用基于三角函数的位置编码，是因为三角函数的和差化积公式。位置编码的相对关系只取决于相对位置而与绝对位置无关，即

$$cos(pos+k)=cos(pos)cos(k)-sin(pos)sin(k)$$

以及

$$sin(pos+k)=sin(pos)cos(k)+cos(pos)sin(k)$$

即两个位置`pos`和`pos+k`的线性变换只依赖与相对距离`k`，这在语言学上是成立的，因为我们更关注分词之间的相对位置关系。当然，这里的公式忽略了位置编码中的分母部分，因为分母是一个常数并不影响上述关系的说明。
- `Embedding`
```python
# 基于transformer原始论文实现的Embedding层，进行了缩放，缩放词嵌入向量的尺度以适配位置编码向量
class Embedding(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = Tensor(d_model)
        self.embedding = nn.Embedding(vocab_size,d_model)

    def forward(self, x):
        return self.embedding(x) * self.d_model.sqrt()
```
基于`transformer`原始论文的实现。在初始化时，`embedding`层通常被初始化为较小的值，如果直接与位置编码向量进行相加可能导致语义信息被位置编码信息掩盖，因此乘一个数来放大词嵌入向量以调整词嵌入向量和位置编码向量的尺度关系。这个`d_model.sqrt()`没有严格的数学来源，而是经验产物。理想情况下,词嵌入向量的尺度应该稍大于位置编码向量的尺度,即词嵌入向量充当语义的主要组成部分,而位置编码信息对语义进行一定程度的修饰作用。
- 其余`block`
```python
class Encoder(nn.Module):
    def __init__(self, num_layers, vocab_size, d_model, num_heads, d_ff, dropout_p, max_seq_len=100, pad_id=-1):
        self.pad_id = pad_id
        self.embedding = Embedding(vocab_size, d_model)
        self.pos_enc = PositionEncoding(d_model, max_seq_len)
        self.layers = [EncoderLayer(d_model, num_heads, d_ff, dropout_p) for _ in range(num_layers)]

    def forward(self, x, mask=None):
        if mask is None:
            mask = self._get_pad_mask(x)
        x = self.pos_enc(self.embedding(x))
        for layer in self.layers:
            x = layer(x, mask)
        return x, mask

    def _get_pad_mask(self, x):
        # x: B,S
        # mask: B,1,S
        data = x.data if isinstance(x, Tensor) else np.asarray(x)
        return np.expand_dims((data==self.pad_id), 1)


class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout_p=0.1):
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout_p)
        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout_p)
        self.ffn = FeedForward(d_model, d_ff, dropout_p)

    def forward(self, tgt, encoder_out, look_ahead_mask=None, src_mask=None):
        tgt = self.self_attn(tgt, tgt, tgt, look_ahead_mask)
        tgt = self.cross_attn(tgt, encoder_out, encoder_out, src_mask)
        return self.ffn(tgt)


class Decoder(nn.Module):
    def __init__(self, num_layers, vocab_size, d_model, num_heads, d_ff, dropout_p, max_seq_len=100, pad_id=-1):
        self.pad_id = pad_id
        self.embedding = Embedding(vocab_size, d_model)
        self.pos_enc = PositionEncoding(d_model, max_seq_len)
        self.layers = [DecoderLayer(d_model, num_heads, d_ff, dropout_p) for _ in range(num_layers)]

    def forward(self, tgt, encoder_out, look_ahead_mask=None, src_mask=None):
        if look_ahead_mask is None:
            look_ahead_mask = self._get_mask(tgt)
        tgt = self.pos_enc(self.embedding(tgt))
        for layer in self.layers:
            tgt = layer(tgt, encoder_out, look_ahead_mask, src_mask)
        return tgt

    def _get_look_ahead_mask(self, x):
        seq_len = np.asarray(x).shape[1]
        return ~np.expand_dims(np.tril(np.ones((seq_len, seq_len)), 0), 0).astype(bool)

    def _get_pad_mask(self, x):
        # x: B,S
        # mask: B,1,S
        data = x.data if isinstance(x, Tensor) else np.asarray(x)
        return np.expand_dims((data == self.pad_id), 1)

    def _get_mask(self, x):
        return self._get_pad_mask(x) | self._get_look_ahead_mask(x)


class Generator(nn.Module):
    def __init__(self, vocab_size, d_model):
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        return self.linear(x)
```
均为经典实现。差别在于将计算掩码的方法封装在块内部实现动态传入`mask`的效果，增强封装性。  
前瞻掩码/因果掩码的原理：在`decoder`层，我们希望每个`token`只基于已有的`token`而不允许得知后续的`token`，因此要将$Q@K^T$的上三角部分（不包括主对角线）掩盖掉，防止模型作弊。

### model.py
- Transformer
```python
class Transformer(Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers,
                 num_heads, d_ff, max_seq_len=100, dropout_p=0.1, pad_id=-1):
        self.encoder = Encoder(num_layers, src_vocab_size, d_model, num_heads,
                               d_ff, dropout_p, max_seq_len, pad_id)
        self.decoder = Decoder(num_layers, tgt_vocab_size, d_model, num_heads,
                               d_ff, dropout_p, max_seq_len, pad_id)
        self.generator = Generator(tgt_vocab_size, d_model)

    def forward(self, src_indices, tgt_indices, src_mask=None, tgt_mask=None):
        encoder_out, src_mask = self.encoder.forward(src_indices, src_mask)
        tgt = self.decoder.forward(tgt_indices, encoder_out, tgt_mask, src_mask)
        return self.generator(tgt)
```
简单地将块进行拼装。
### main.py
```python
import numpy as np
from autograd.loss import CrossEntropyLoss
from autograd.optim import SGD
from transformer.model import Transformer

# 设置超参数
SRC_VOCAB = 10
TGT_VOCAB = 12
D_MODEL = 8
D_FF = 16
NUM_HEADS = 2
NUM_LAYERS = 2
MAX_LEN = 10
BATCH_SIZE = 1
SRC_LEN = 5
TGT_LEN = 6
NUM_EPOCHS = 1000
LEARNING_RATE = 0.01
PRINT_EVERY = 10  # 每 10 个 epoch 打印一次日志
PAD_ID = -1

# 创建模型
model = Transformer(SRC_VOCAB, TGT_VOCAB, D_MODEL, NUM_LAYERS,
                    NUM_HEADS, D_FF, MAX_LEN, 0.1, PAD_ID)

# 创建损失函数和优化器
criterion = CrossEntropyLoss(ignore_index=PAD_ID)
optimizer = SGD(model.parameters(), lr=LEARNING_RATE)

# 虚拟样本
src_indices = np.random.randint(1, SRC_VOCAB, size=(BATCH_SIZE, SRC_LEN))
tgt_indices = np.random.randint(1, TGT_VOCAB, size=(BATCH_SIZE, TGT_LEN))  # 输入decoder的数据

targets = np.roll(tgt_indices, -1, axis=-1)  # 做损失的数据，与tgt_indices有一个相位差
targets[:, -1] = PAD_ID  # 最后一个设为 <pad>

# 开始训练
model.train()
total_loss = 0.0
for epoch in range(NUM_EPOCHS):
    logits = model(src_indices, tgt_indices)
    loss = criterion(logits, targets)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    total_loss += loss.data  # 累加损失

    # 12. 打印日志
    if (epoch + 1) % PRINT_EVERY == 0:
        avg_loss = total_loss / PRINT_EVERY
        print(f"Epoch [{epoch + 1}/{NUM_EPOCHS}], 平均 Loss: {avg_loss:.4f}")
        total_loss = 0.0
```
由于手写框架`api`与`pytorch`的高度一致性，这里的训练代码与简单的`pytorch`训练代码完全一致。
```text
E:\anaconda3\python.exe "E:\PyCharm 2025.1.1\Projects\numpy_transformer\main.py" 
Epoch [10/1000], 平均 Loss: 3.4225
Epoch [20/1000], 平均 Loss: 2.8649
Epoch [30/1000], 平均 Loss: 2.3359
Epoch [40/1000], 平均 Loss: 2.1415
Epoch [50/1000], 平均 Loss: 1.9099
Epoch [60/1000], 平均 Loss: 1.8141
Epoch [70/1000], 平均 Loss: 1.7073
Epoch [80/1000], 平均 Loss: 1.5506
Epoch [90/1000], 平均 Loss: 1.4054
Epoch [100/1000], 平均 Loss: 1.2989
Epoch [110/1000], 平均 Loss: 1.1887
Epoch [120/1000], 平均 Loss: 1.1033
Epoch [130/1000], 平均 Loss: 1.0571
Epoch [140/1000], 平均 Loss: 0.9574
Epoch [150/1000], 平均 Loss: 0.9499
Epoch [160/1000], 平均 Loss: 0.8263
Epoch [170/1000], 平均 Loss: 0.7892
Epoch [180/1000], 平均 Loss: 0.7088
Epoch [190/1000], 平均 Loss: 0.6879
Epoch [200/1000], 平均 Loss: 0.5882
Epoch [210/1000], 平均 Loss: 0.5733
Epoch [220/1000], 平均 Loss: 0.5052
Epoch [230/1000], 平均 Loss: 0.4772
Epoch [240/1000], 平均 Loss: 0.5217
Epoch [250/1000], 平均 Loss: 0.5071
Epoch [260/1000], 平均 Loss: 0.4223
Epoch [270/1000], 平均 Loss: 0.4532
Epoch [280/1000], 平均 Loss: 0.3859
Epoch [290/1000], 平均 Loss: 0.3218
Epoch [300/1000], 平均 Loss: 0.3518
Epoch [310/1000], 平均 Loss: 0.2785
Epoch [320/1000], 平均 Loss: 0.2920
Epoch [330/1000], 平均 Loss: 0.3094
Epoch [340/1000], 平均 Loss: 0.2513
Epoch [350/1000], 平均 Loss: 0.2440
Epoch [360/1000], 平均 Loss: 0.2546
Epoch [370/1000], 平均 Loss: 0.2979
Epoch [380/1000], 平均 Loss: 0.2456
Epoch [390/1000], 平均 Loss: 0.1858
Epoch [400/1000], 平均 Loss: 0.1483
Epoch [410/1000], 平均 Loss: 0.1963
Epoch [420/1000], 平均 Loss: 0.1836
Epoch [430/1000], 平均 Loss: 0.2244
Epoch [440/1000], 平均 Loss: 0.1308
Epoch [450/1000], 平均 Loss: 0.1789
Epoch [460/1000], 平均 Loss: 0.1576
Epoch [470/1000], 平均 Loss: 0.2051
Epoch [480/1000], 平均 Loss: 0.1337
Epoch [490/1000], 平均 Loss: 0.1972
Epoch [500/1000], 平均 Loss: 0.1906
Epoch [510/1000], 平均 Loss: 0.1289
Epoch [520/1000], 平均 Loss: 0.1030
Epoch [530/1000], 平均 Loss: 0.1425
Epoch [540/1000], 平均 Loss: 0.0858
Epoch [550/1000], 平均 Loss: 0.1192
Epoch [560/1000], 平均 Loss: 0.1270
Epoch [570/1000], 平均 Loss: 0.1009
Epoch [580/1000], 平均 Loss: 0.1696
Epoch [590/1000], 平均 Loss: 0.1340
Epoch [600/1000], 平均 Loss: 0.1008
Epoch [610/1000], 平均 Loss: 0.2158
Epoch [620/1000], 平均 Loss: 0.0828
Epoch [630/1000], 平均 Loss: 0.0858
Epoch [640/1000], 平均 Loss: 0.1516
Epoch [650/1000], 平均 Loss: 0.0665
Epoch [660/1000], 平均 Loss: 0.1029
Epoch [670/1000], 平均 Loss: 0.1480
Epoch [680/1000], 平均 Loss: 0.1109
Epoch [690/1000], 平均 Loss: 0.0613
Epoch [700/1000], 平均 Loss: 0.0630
Epoch [710/1000], 平均 Loss: 0.0607
Epoch [720/1000], 平均 Loss: 0.0569
Epoch [730/1000], 平均 Loss: 0.1581
Epoch [740/1000], 平均 Loss: 0.0581
Epoch [750/1000], 平均 Loss: 0.0701
Epoch [760/1000], 平均 Loss: 0.1562
Epoch [770/1000], 平均 Loss: 0.1247
Epoch [780/1000], 平均 Loss: 0.0748
Epoch [790/1000], 平均 Loss: 0.0581
Epoch [800/1000], 平均 Loss: 0.0443
Epoch [810/1000], 平均 Loss: 0.0679
Epoch [820/1000], 平均 Loss: 0.0537
Epoch [830/1000], 平均 Loss: 0.0721
Epoch [840/1000], 平均 Loss: 0.0733
Epoch [850/1000], 平均 Loss: 0.0582
Epoch [860/1000], 平均 Loss: 0.0612
Epoch [870/1000], 平均 Loss: 0.0784
Epoch [880/1000], 平均 Loss: 0.0417
Epoch [890/1000], 平均 Loss: 0.0980
Epoch [900/1000], 平均 Loss: 0.0726
Epoch [910/1000], 平均 Loss: 0.0643
Epoch [920/1000], 平均 Loss: 0.1285
Epoch [930/1000], 平均 Loss: 0.1339
Epoch [940/1000], 平均 Loss: 0.0485
Epoch [950/1000], 平均 Loss: 0.0479
Epoch [960/1000], 平均 Loss: 0.0330
Epoch [970/1000], 平均 Loss: 0.0642
Epoch [980/1000], 平均 Loss: 0.0335
Epoch [990/1000], 平均 Loss: 0.0798
Epoch [1000/1000], 平均 Loss: 0.1778

进程已结束，退出代码为 0
```
损失正常下降，模型正常过拟合(因为只有一个样本)，说明框架以及模型的实现无误。  

transformer部分的代码解析完毕。
